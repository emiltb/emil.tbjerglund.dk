<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Open and Efficient Research</title>
    <link>/categories/r/</link>
    <description>Recent content in R on Open and Efficient Research</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Emil Tveden Bjerglund</copyright>
    <lastBuildDate>Fri, 04 Aug 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/r/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Combining R and Python for data analysis</title>
      <link>/post/combining-r-and-python-for-data-analysis/</link>
      <pubDate>Fri, 04 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/combining-r-and-python-for-data-analysis/</guid>
      <description>&lt;p&gt;As part of my PhD work I characterise nanomaterials using &lt;a href=&#34;https://en.wikipedia.org/wiki/Energy-dispersive_X-ray_spectroscopy&#34;&gt;Energy-dispersive X-ray spectroscopy (EDX)&lt;/a&gt; in a Scanning Transmission Electron Microscope. We do this to obtain spatial information about the chemical composition of a sample on the nanoscale. Basically, an image is obtained by raster-scanning the electron beam and recording an X-ray spectrum in each position. This effectively gives a 3-dimensional dataset, where for each pixel a full spectrum is recorded. Since different elements emit X-rays at different energies, we can essentially make images of each element in a sample by extracting that part of the spectrum.&lt;/p&gt;
&lt;p&gt;One of the best tools I have encountered for analysing such datasets is &lt;a href=&#34;http://hyperspy.org/&#34;&gt;HyperSpy&lt;/a&gt;. It is a Python library containing easy-to-use methods for interactively exploring and extracting features. As an example it is very easy to load a datafile and visualise the spectrum at each point.&lt;/p&gt;
&lt;video width=&#34;100%&#34; controls autoplay loop&gt;
&lt;source src=&#34;/img/Hyperspy.webm&#34; type=&#34;video/webm&#34;&gt; Your browser does not support the video tag.
&lt;/video&gt;
&lt;p&gt;Now, since HyperSpy is a Python library this obviously brings some headaches, when my preferred analysis tool is R. I could do the entire analysis in a &lt;a href=&#34;http://jupyter.org/&#34;&gt;Jupyter Notebook&lt;/a&gt;, and use matplotlib to generate pretty figures of the results. EDX data is only a small part of the data I work with, and all other data is analysed and plottd in R Markdown documents. This means, that I could quickly end up producing figures with inconsistent looks when displayed alongside other data.&lt;/p&gt;
&lt;p&gt;Fortunately it is possible to run Python code in an R Markdown document by &lt;a href=&#34;http://rmarkdown.rstudio.com/authoring_knitr_engines.html&#34;&gt;using a different language engine&lt;/a&gt;. This means that I can load HyperSpy from my Rmd file, run the Python code for the analysis, and then continue in a new code chunk containing R code. (Note that ’ should be replaced by ` below).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;#39;&amp;#39;&amp;#39;{python}
#import hyperspy.api as hs
&amp;#39;&amp;#39;&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The main drawback of doing it this way is that I am losing on the interactive explorative tools included in HyperSpy. For this reason my workflow has been to interactively explore and develop the Python code in a Jupyter Notebook, and then copy the final script to a Python chunk in an R Markdown document. I do this to keep the final product together and get a completely reproducible analysis in my R Markdown document.&lt;/p&gt;
&lt;div id=&#34;transferring-data-between-python-and-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Transferring data between Python and R&lt;/h2&gt;
&lt;p&gt;The datafile used in this example is a Bruker composite file (.bcf) containing a 512x512 pixel image with 2048 channels in the energy spectrum. I use HyperSpy to integrate specific elemental peaks in the spectrum. This produces maps of each element in the sample. I would like to plot these in R, as I am more proficient at fine-tuning the plots to my desired look in ggplot2. It is however not possible to directly transfer data from a Python code chunk to an R code chunk, so I need to save the data to the disk and and then load it again. A specific package, &lt;a href=&#34;https://blog.rstudio.com/2016/03/29/feather/&#34;&gt;feather&lt;/a&gt;, was developed to transfer data frames between Python and R. However, since my data is just images, I find it easier to just save them as .tiff files and read these into R. This gives me the following Python script. If you are curious about analysing hyperspectral data with HyperSpy please see &lt;a href=&#34;http://hyperspy.org/hyperspy-doc/current/user_guide/index.html&#34;&gt;the documentation&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;# Load Hyperspy
import hyperspy.api as hs

# Define where to find the data
filename = &amp;#39;data/Co sample map.bcf&amp;#39;
folder = &amp;#39;data/&amp;#39;
sample = &amp;#39;Co&amp;#39;

# Read the raw datafile (.bcf)
s = hs.load(filename)[1]
print(s)

# The signal is low - rebin the data and then integrate the peaks
raw_result = s.rebin(new_shape=[256,256,2048]).get_lines_intensity()

# For each element save the result as an image
for j in range(len(raw_result)):
    fn = &amp;#39;&amp;#39;.join([folder, sample, &amp;#39;-&amp;#39;, raw_result[j].metadata.Sample.xray_lines[0]])
    raw_result[j].as_signal2D(image_axes=[0,1]).save(
      filename = fn, 
      extension = &amp;quot;tiff&amp;quot;, 
      overwrite = True
    )

# Save the scale of the image
file = open(&amp;#39;&amp;#39;.join([folder, sample,&amp;#39;_scale.txt&amp;#39;]), &amp;quot;w&amp;quot;)
print(s.axes_manager[&amp;quot;width&amp;quot;].scale, file=file)
file.close()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;EDSTEMSpectrum, title: EDX, dimensions: (512, 512|2048)&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can examine the data folder from R, and see that the images were saved.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dir(&amp;quot;data/&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Co-C_Ka.tiff&amp;quot;      &amp;quot;Co-Ca_Ka.tiff&amp;quot;     &amp;quot;Co-Co_Ka.tiff&amp;quot;    
## [4] &amp;quot;Co-Mo_Ka.tiff&amp;quot;     &amp;quot;Co-Na_Ka.tiff&amp;quot;     &amp;quot;Co-O_Ka.tiff&amp;quot;     
## [7] &amp;quot;Co-S_Ka.tiff&amp;quot;      &amp;quot;Co sample map.bcf&amp;quot; &amp;quot;Co_scale.txt&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I put this vector of filenames into a tibble and read the corresponding scale (to be used later). The approach here might be overkill, but it makes it very easy to add more paths and load data from several files simultaneously.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

samples &amp;lt;- tibble(path = &amp;quot;data/&amp;quot;) %&amp;gt;% 
  mutate(filename = map(path, dir, pattern = &amp;quot;*.tiff&amp;quot;)) %&amp;gt;% 
  mutate(scale = map(path, dir, pattern = &amp;quot;[A-Za-z_]*scale.txt&amp;quot;)) %&amp;gt;% 
  mutate(scale = map(paste0(path,scale), read_lines) %&amp;gt;% as.double() * 1000) %&amp;gt;% 
  unnest()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-tiff-images-in-r&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Reading tiff images in R&lt;/h2&gt;
&lt;p&gt;The generated tiff images can then be loaded using the &lt;code&gt;raster&lt;/code&gt; package as demonstrated below. I also spend quite I few lines of code extracting sample name and elements from the filenames, as well as wrangling the data into a long format, suitable for plotting. In the end I normalise the intensities within each map, since &lt;a href=&#34;https://stackoverflow.com/questions/17006251/vary-the-fill-scale-when-using-facet-wrap-and-geom-tile-together/42934670#42934670&#34;&gt;it is rather inconvenient to vary the fill scale when using &lt;code&gt;facet_wrap()&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(raster)
library(stringr)
library(forcats)

maps &amp;lt;- samples %&amp;gt;%
  separate(filename, c(&amp;quot;sample&amp;quot;, &amp;quot;element&amp;quot;), &amp;quot;-&amp;quot;, remove = FALSE) %&amp;gt;% 
  mutate(element = str_extract(element,&amp;quot;^[A-Za-z_]*&amp;quot;)) %&amp;gt;% 
  separate(element, c(&amp;quot;element&amp;quot;, &amp;quot;edge&amp;quot;), &amp;quot;_&amp;quot;) %&amp;gt;% 
  mutate(img = map(paste0(path, .data$filename), raster)) %&amp;gt;%
  mutate(img = map(img, as, &amp;quot;SpatialPixelsDataFrame&amp;quot;)) %&amp;gt;%
  mutate(img = map(img, as_tibble)) %&amp;gt;%
  unnest() %&amp;gt;% 
  mutate(x = x*scale, y = y*scale) %&amp;gt;% 
  dplyr::select(-sample, -edge, -path, -filename) %&amp;gt;% 
  gather(key = &amp;quot;map&amp;quot;, value = &amp;quot;intensity&amp;quot;, -x, -y, -element) %&amp;gt;% 
  na.omit() %&amp;gt;% 
  dplyr::select(element, x, y, intensity) %&amp;gt;% 
  filter(element %in% c(&amp;quot;C&amp;quot;, &amp;quot;Co&amp;quot;, &amp;quot;Mo&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;O&amp;quot;)) %&amp;gt;%
  mutate(element = fct_relevel(element, &amp;quot;C&amp;quot;, &amp;quot;O&amp;quot;, &amp;quot;S&amp;quot;, &amp;quot;Mo&amp;quot;, &amp;quot;Co&amp;quot;)) %&amp;gt;% 
  group_by(element) %&amp;gt;% 
  mutate(intnorm = (intensity - min(intensity)) / (max(intensity) - min(intensity))) %&amp;gt;% 
  ungroup()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-5&#34;&gt;Table 1: &lt;/span&gt;maps tibble after wrangling&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;element&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;x&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;y&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;intensity&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;intnorm&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.5256356&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;268.5998&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.051271&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0477851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.5769069&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;268.5998&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.051271&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0477851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.6281782&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;268.5998&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.051271&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0477851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.6794494&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;268.5998&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.051271&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0477851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4.7307207&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;268.5998&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.051271&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0477851&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;C&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.7819920&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;268.5998&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.051271&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.0477851&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From here it is very easy to plot the different elemental maps using ggplot2. I use the &lt;a href=&#34;https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html&#34;&gt;viridis&lt;/a&gt; color scale here for its many good qualities.&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(data = maps, aes(x = x, y = y, fill = intnorm)) +
  geom_raster() +
  facet_wrap(~element, nrow = 1) +
  coord_equal() +
  viridis::scale_fill_viridis() +
  theme_minimal() +
  theme(legend.position = &amp;quot;none&amp;quot;) +
  labs(x = &amp;quot;x [nm]&amp;quot;, y = &amp;quot;y [nm]&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-08-04-combining-r-and-python-for-data-analysis_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;final-thoughts&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;The goal of this post was to demonstrate, that a mixed analysis in R and Python can be kept in one place by working in R Markdown documents. An obvious alternative method would be to install &lt;a href=&#34;https://github.com/IRkernel/IRkernel&#34;&gt;an R kernel for Jupyter&lt;/a&gt; or &lt;a href=&#34;http://rpy.sourceforge.net/&#34;&gt;rpy2&lt;/a&gt;, which also makes it possible to mix R and Python in one notebook. This could be useful for avid Python users wishing to bring functionality from a powerful R package to their analysis.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Viridis looks great, transitions smoothly between colors, reproduces nicely in greyscale and is easier to read for colorblind people.&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Interactive data analysis using shinyGadgets</title>
      <link>/post/interactive-data-analysis-using-shinygadgets/</link>
      <pubDate>Sat, 22 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/interactive-data-analysis-using-shinygadgets/</guid>
      <description>&lt;p&gt;One of the main points I made in my last post was, that one of the barriers of adopting R as the main analysis tool is that people need accesible tools that makes it easy to do routine jobs very easy. In this post I will demonstrate how shinyGadgets can be used interactively in an analysis.&lt;/p&gt;
&lt;div id=&#34;routine-jobs-in-r-made-easy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Routine jobs in R made easy&lt;/h1&gt;
&lt;p&gt;One of the jobs, that is often performed in the group is integrating cyclic voltammetry peaks to know the charge passed in a single sweep, since that tells us how many electroactive groups that reacted during one sweep. This is not necessarily easy, since there often is a large background current that needs to be substracted. We previously have had other tools (self-made software, Origin, etc.) but this is not necessarily easy.&lt;/p&gt;
&lt;p&gt;I developed an R-function to make an easier interface to do this. Here a dataframe is loaded with the cv data, and that can then be passed to an &lt;code&gt;area()&lt;/code&gt; function where the sweep number, integration limits and the order of the background polynomial can be specified.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# library(osc)
# file &amp;lt;- system.file(&amp;quot;extdata/cv/cv_example.txt&amp;quot;, package = &amp;quot;osc&amp;quot;)
# df &amp;lt;- echem_read(file)
# df &amp;lt;- area(df, sw = 1, x1 = -1.8, x2 = -1.4, p = 3)
# plot_area(df)&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;using-interactive-shinygadgets-to-generate-analysis-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Using interactive shinyGadgets to generate analysis code&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Setting up an RStudio Server for teaching</title>
      <link>/post/setting-up-an-rstudio-server-for-teaching/</link>
      <pubDate>Wed, 12 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/setting-up-an-rstudio-server-for-teaching/</guid>
      <description>&lt;p&gt;Installed ubuntu server 16.04 on an old laptop. After initial setup, login with the user created during install.&lt;/p&gt;

&lt;p&gt;For good measure, install the updates. Well that didn&amp;rsquo;t work, since Ubuntu 16.04 apparantly has problems with my network card (&lt;a href=&#34;https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1586875&#34; target=&#34;_blank&#34;&gt;https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1586875&lt;/a&gt;). I did not find a way to resolve that.&lt;/p&gt;

&lt;p&gt;Well, tried again with Ubuntu Server 17.04, which also did not help - at this point it is probably my knowledge on Linux systems that is the limiting factor. I instead installed the normal Ubuntu Desktop 17.04, and then both Wired and Wireless connections worked just fine.&lt;/p&gt;

&lt;p&gt;I then followed this guide: &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-set-up-r-on-ubuntu-14-04&#34; target=&#34;_blank&#34;&gt;https://www.digitalocean.com/community/tutorials/how-to-set-up-r-on-ubuntu-14-04&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo nano /etc/apt/sources.list&lt;/code&gt; and added the line &lt;code&gt;deb https://cran.rstudio.com/bin/linux/ubuntu zesty/&lt;/code&gt; to the bottom. Add key by &lt;code&gt;sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E084DAB9&lt;/code&gt; (&lt;a href=&#34;https://cran.rstudio.com/bin/linux/ubuntu/README.html&#34; target=&#34;_blank&#34;&gt;https://cran.rstudio.com/bin/linux/ubuntu/README.html&lt;/a&gt;). Save and run &lt;code&gt;sudo apt-get update&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;sudo apt-get install r-base&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install gdebi-core
wget https://download2.rstudio.org/rstudio-server-1.0.143-amd64.deb
sudo gdebi rstudio-server-1.0.143-amd64.deb
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After that, the Rstudio server was installed and running. I could go to &lt;ip&gt;:8787 on another computer on the local network and log in with my user.&lt;/p&gt;

&lt;p&gt;As I wish to use this server for teaching, I need to make a bunch of packages, e.g. the tidyverse available for all users. So I start R on the server as a superuser by &lt;code&gt;sudo R&lt;/code&gt; and then run &lt;code&gt;install.packages(&#39;tidyverse&#39;, lib = &#39;/usr/local/lib/R/site-library/)&lt;/code&gt;. This takes a while, and the first times I did this some packages failed (rvest, httr, xml2, curl, openssl) due to missing dependencies. These were installed by&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install libxml2-dev
sudo apt-get install libcurl4-openssl-dev
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;After which the install of tidyverse progressed without errors&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo useradd -m andschmidt
sudo passwd andsmidt # Supply 123abc
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To delete a user &lt;code&gt;sudo userdel andschmidt&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Science tools for our research group</title>
      <link>/post/open-science-tools-for-our-research-group/</link>
      <pubDate>Mon, 10 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/open-science-tools-for-our-research-group/</guid>
      <description>&lt;p&gt;Currently in the &lt;a href=&#34;//surfchem.dk&#34;&gt;Organic Surface Chemistry group&lt;/a&gt;, there are large variations between our group members when it comes to the tools used for data analysis. Some people feel most comfortable in spreadsheet programs such as Origin or Excel, while others rely on a mix of Matlab, R, Python or other tools. We use a lot of different experimental techniques in our research, and therefore generate data in a lot of different formats. Different workflows of varying efficiency often makes joint projects more difficult than necessary.&lt;/p&gt;
&lt;div id=&#34;open-science&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Open Science&lt;/h2&gt;
&lt;p&gt;Recently a lot of our research activities has been oriented towards the &lt;a href=&#34;//www.spoman-os.org&#34;&gt;SPOMAN Open Science collaboration&lt;/a&gt;, and the ongoing projects are available on the &lt;a href=&#34;//osf.io/wudyt&#34;&gt;Open Science Framework (osf.io)&lt;/a&gt;. In most cases, the methods and results are thoroughly described for each project. However, since a lot of the data analysis is currently performed in spreadsheet software, the data analysis is not very transparent and reproducible. &lt;span class=&#34;citation&#34;&gt;Lowndes et al. (2017)&lt;/span&gt; discusses exactly this issue, and describes how the use of &lt;a href=&#34;//www.r-project.org&#34;&gt;R&lt;/a&gt; and &lt;a href=&#34;//github.com&#34;&gt;GitHub&lt;/a&gt; has allowed them to do better science in less time.&lt;/p&gt;
&lt;p&gt;I have been considering how to apply this thinking to our research. I am confident that it could be made more efficient and well-documented if R was adopted as a general tool for data analysis.&lt;/p&gt;
&lt;div id=&#34;clear-and-well-documented-analysis-in-r-markdown&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Clear and well-documented analysis in R Markdown&lt;/h3&gt;
&lt;p&gt;If you recieve a spreadsheet from someone, containing some data analysis it can be very hard to decipher the thinking of the original author (and to be honest, analysis done by yourself 6 monts ago might as well have been done by a complete stranger). Some of the challenges are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is no connection to the original data. How did the data come from your scientific equipment and into the spreadsheet? Was it in any way altered previously?&lt;/li&gt;
&lt;li&gt;What calculations are performed? Which cells contain data and formulas? You have to manually click around to know.&lt;/li&gt;
&lt;li&gt;There is no easy way to reproduce an analysis, without having to enter new formulas, making new assumptions etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;R provides a very nice feature which can alleviate these challenges, and make an analysis easier to understand and reproduce: &lt;a href=&#34;//rmarkdown.rstudio.com&#34;&gt;R Markdown documents&lt;/a&gt;. &lt;em&gt;This post is written in R Markdown to demonstrate some of the capabilities (you can go &lt;a href=&#34;https://github.com/emiltb/emil.tbjerglund.dk/tree/master/content/post&#34;&gt;here&lt;/a&gt; to see the code)&lt;/em&gt;&lt;a href=&#34;#fn1&#34; class=&#34;footnoteRef&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. It makes it easy to alternate between descriptive paragraphs, describing the analysis and intermediate conclusions, and then R code, where scripts are used to load, modify and plot data.&lt;/p&gt;
&lt;p&gt;We could for example provide the script for loading a dataset and directly show how a few of the variables are modified. This is demonstrated here for R’s built-in &lt;code&gt;mtcars&lt;/code&gt; dataset.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)
dataset &amp;lt;- mtcars %&amp;gt;% 
  mutate(wt = wt*0.45359,      # Convert lbs to tons
         mpg = mpg * 0.42514,  # Convert mpg to km/L
         Transmission = forcats::fct_recode(as.factor(am), 
                                            Automatic = &amp;quot;0&amp;quot;, 
                                            Manual = &amp;quot;1&amp;quot;)
    )

knitr::kable(
  head(dataset, 4), 
  caption = &amp;quot;The mtcars dataset contain data on 32 different cars. Here 4 rows are shown.&amp;quot;
)&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:data-modify&#34;&gt;Table 1: &lt;/span&gt;The mtcars dataset contain data on 32 different cars. Here 4 rows are shown.&lt;/caption&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;car&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mpg&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;cyl&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;hp&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;wt&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;am&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;Transmission&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mazda RX4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.927940&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;110&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.188406&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manual&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Mazda RX4 Wag&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8.927940&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;110&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.304071&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manual&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Datsun 710&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.693192&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.052329&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Manual&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Hornet 4 Drive&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.097996&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;110&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.458292&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Automatic&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We can then plot our dataset, in a completely reproducible manner by a few lines of code. Anyone with the datafile and this script, would be able to produce this exact plot. Notice that the package &lt;code&gt;ggplot2&lt;/code&gt; is used, producing publication-ready figures right away.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dataset %&amp;gt;%  
  ggplot(aes(x = wt, y = mpg, color = Transmission)) +
  geom_point() +
  labs(x = &amp;quot;Weight (t)&amp;quot;, y = &amp;quot;Mileage (km/L)&amp;quot;, title = &amp;quot;Fuel economy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-07-07-open-science-tools-for-our-research-group_files/figure-html/data-plot-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;reuse-of-work-through-an-r-package&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Reuse of work through an R package&lt;/h3&gt;
&lt;p&gt;Take any given &lt;a href=&#34;http://surfchem.dk/category/articles/&#34;&gt;research project&lt;/a&gt; in our group, and you will commonly find that we use X-ray Photoelectron Spectroscopy (XPS), Raman spectroscopy, Infrared spectroscopy, Ellipsommetry and various electrochemical techniques all the time. In essence, a handful of techniques produce the large majority of our data. Most of it is initially saved in proprietary formats and treated in the corresponding software, but it can all be exported as plain-text files (.csv, .txt). Wouldn’t it be nice to have a common framework to analyse and plot all data, for every project, regardless of the technique used? This is also possible in R by creating a package!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Packages are the fundamental units of reproducible R code. They include reusable R functions, the documentation that describes how to use them, and sample data. &lt;a href=&#34;//r-pkgs.had.co.nz/&#34;&gt;&lt;strong&gt;R packages&lt;/strong&gt;, &lt;em&gt;Hadley Wickham&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Initially, an R package for our group could contain functions to load the data from the above-mentioned techniques and clean it up, so that it is ready for analysis. We could then extend on this by providing often-used analyses as functions, shortcuts for making commonly used plots etc. In this way code could easily be reused among members of the group, making our science more efficient. Having such a toolbox provided along with some basic examples will also make it easier for new group members to get started with R.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;collaborative-todo-lists-and-dicussions-on-github&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Collaborative TODO-lists and dicussions on GitHub&lt;/h3&gt;
&lt;p&gt;When you have then made your R package, how is it then distributed among group members? To be truly efficient we need a tool that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Easily allows users to download the newest version of the package when using R.&lt;/li&gt;
&lt;li&gt;Allows for cooperation from many users, such that the maintainence and debugging the code is independent of any single person.&lt;/li&gt;
&lt;li&gt;Contains version control, such that users can append their own code to the project or modify other peoples code without fear of breaking stuff.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All this is possible, if the package is made publically available as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Git&#34;&gt;Git repository&lt;/a&gt;. As of writing this, I have created an empty R package that is available &lt;a href=&#34;https://github.com/SPOMAN/osc&#34;&gt;on GitHub&lt;/a&gt;, and the hope is that in due time it will be populated with lots of useful stuff for our group members. This solution makes the code publically available and very easy to use in R.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Install the package from Github using the Devtools package
install.packages(&amp;#39;devtools&amp;#39;)
devtools::install_github(&amp;#39;SPOMAN/osc&amp;#39;)

# Load the &amp;#39;osc&amp;#39; package
library(osc)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Github further allows users and developers to &lt;a href=&#34;https://github.com/SPOMAN/osc/issues&#34;&gt;track &lt;em&gt;issues&lt;/em&gt;&lt;/a&gt;. This can be used as a TODO-list for the project to report bugs in the code or request missing features. This means, that even R users that mainly use the scripts, and might be unable to add new functionality themselves, has the possibility to help improve the package.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;key-steps-for-implementation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Key steps for implementation&lt;/h2&gt;
&lt;p&gt;It is clear, that there is an inital barrier to overcome, if the implementation of R as the primary analysis tool in our group is to succeed. Currently many students in our group have very limited experience in programming, but I believe that by implementing core tools in an accessible environment this can be overcome. Teaching R to new users was discussed extensively at the useR!2017 conference I attended last week. Some materials are available in the excellent talk &lt;a href=&#34;https://github.com/mine-cetinkaya-rundel/2017-07-05-teach-ds-to-new-user&#34;&gt;“Teaching data science to new useRs” by Mine Cetinkaya-Rundel&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In the long run I am confident that the extra workload that must be put into making a well-crafted R package will pay off, once the scientific analyses can be performed more efficient and reproducibly by every member of our research group.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level3 unnumbered&#34;&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;
&lt;div id=&#34;ref-Lowndes2017&#34;&gt;
&lt;p&gt;Lowndes, Julia S. Stewart, Benjamin D. Best, Courtney Scarborough, Jamie C. Afflerbach, Melanie R. Frazier, Casey C. O’Hara, Ning Jiang, and Benjamin S. Halpern. 2017. “Our path to better science in less time using open data science tools.” &lt;em&gt;Nature Ecology &amp;amp; Evolution&lt;/em&gt; 1 (6): 0160. doi:&lt;a href=&#34;https://doi.org/10.1038/s41559-017-0160&#34;&gt;10.1038/s41559-017-0160&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Fun fact: R Markdown can also be used to product PDFs, Word documents or even slideshows!&lt;a href=&#34;#fnref1&#34;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
